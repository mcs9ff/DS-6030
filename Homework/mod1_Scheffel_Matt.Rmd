---
title: "DS-6030 Homework Module 1"
author: "Matt Scheffel"
output:
  pdf_document:
    toc: no
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->


```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE)     # do not show R messages
```

<!--- Change font size for headers --->
<style>
h1.title { font-size: 28px; }
h1 { font-size: 22px; }
h2 { font-size: 18px; }
h3 { font-size: 14px; }
</style>

**DS 6030 | Spring 2022 | University of Virginia **

# 1. Flexible vs Inflexible Methods
For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

(a) The sample size $n$ is extremely large, and the number of predictors $p$ is small.

For this example, we would expect the performance of a flexible statistical learning method to be better than an inflexible method. This is because when a large dataset is present, a flexible method will fit the data better and come closer to its true distribution.

(b) The number of predictors $p$ is extremely large, and the number of observations $n$ is small.

For this example, we would expect the performance of a flexible statistical learning method to be worse than an inflexible method. This is due to the issue of overfitting with the smaller dataset.

(c) The relationship between the predictors and response is highly non-linear.

For this example, we would expect the performance of a flexible statistical learning method to be better than an inflexible method. This is because when there are more degrees of freedom, a flexible method fits the dataset better.

(d) The variance of the error terms, i.e. $\sigma^2 = Var(\epsilon)$, is extremely high.

For this example, we would expect the performance of a flexible statistical learning method to be worse than an inflexible method. This is due to the issue of overfitting with the “noise” of the error terms having a large impact on the fit.

# 2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$.

(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

This is a regression problem where we are most interested in inference. 

N = 500 and P = 3

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each prod- uct we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

This is a classification problem where we are most interested in prediction. 

N = 20 and P = 14

(c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

This is a regression problem where we are most interested in prediction. 

N = 52 and P = 4

# 6. Describe the differences between a parametric and a non-parametric statistical learning approach. 

What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?

A parametric statistical learning approach assumes a linear function for the model when estimating fit. A non-parametric model makes no assumption, but thus requires a larger sample size. This demonstrates an advantage of the parametric model (in comparison to a non-parametric model): it requires less data/ a smaller sample size. However, a disadvantage is that it may assume the wrong form of the model and result in overfitting that leads to an inaccurate estimate.

# 8. This exercise relates to the College data set, which can be found in the file College.csv on the book website.

It contains a number of variables for 777 different universities and colleges in the US. The variables are

- `Private` : Public/private indicator
- `Apps` : Number of applications received
- `Accept` : Number of applicants accepted
- `Enroll` : Number of new students enrolled
- `Top10perc` : New students from top 10 % of high school class 
- `Top25perc` : New students from top 25 % of high school class 
- `F.Undergrad` : Number of full-time undergraduates
- `P.Undergrad` : Number of part-time undergraduates
- `Outstate` : Out-of-state tuition
- `Room.Board` : Room and board costs
- `Books` : Estimated book costs
- `Personal` : Estimated personal spending
- `PhD` : Percent of faculty with Ph.D.’s
- `Terminal` : Percent of faculty with terminal degree • S.F.Ratio : Student/faculty ratio
- `perc.alumni` : Percent of alumni who donate
- `Expend` : Instructional expenditure per student
- `Grad.Rate` : Graduation rate

Before reading the data into R, it can be viewed in Excel or a text editor.

(a) Use the `read.csv()` function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.

```{r}
setwd("~/Desktop/MSDS/DS 6030/ALL CSV FILES - 2nd Edition")
college <- read.csv("College.csv")
head(college)
```


(b) Look at the data using the `View()` function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:

```{r}
rownames(college) <- college[, 1]
View(college)
```


  You should see that there is now a `row.names` column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try

```{r}
college <- college[, -1]
View(college)
```


  Now you should see that the first data column is `Private`. Note that another column labeled `row.names` now appears before the `Private` column. However, this is not a data column but rather the name that R is giving to each row.
  
(c) 
  i. Use the `summary()` function to produce a numerical summary of the variables in the data set.
  
```{r}
summary(college)
```
  
  
  ii. Use the `pairs()` function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix `A` using `A[,1:10]`.
  
```{r}
pairs(college[,2:10])
```
  
  
  iii. Use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Private`.
  
```{r}
library(ggplot2)

ggplot(college, aes(x = Private, y = Outstate))+
  geom_boxplot()
```
  
  
  iv. Create a new qualitative variable, called `Elite`, by binning the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.

```{r}
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
```

Use the `summary()` function to see how many elite universities there are. Now use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Elite`. 

```{r}
summary(college$Elite)
```

```{r}
ggplot(college, aes(x = Elite, y = Outstate))+
  geom_boxplot()
```


  v. Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow = c(2, 2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.
  
```{r}
hist(college$Outstate)
```
  
```{r}
hist(college$PhD)
```
  
```{r}
par(mfrow = c(2,2))
hist(college$Books, col = 2)
hist(college$PhD, col = 3)
hist(college$Grad.Rate, col = 4)
hist(college$perc.alumni, col = 5)
```
  
  
  vi. Continue exploring the data, and provide a brief summary of what you discover.

I discovered a number of things from this dataset. Public schools tend to have higher raw numbers than private schools. Schools labeled as "Elite" unsurprisingly perform better in many categories.

# 10. This exercise involves the Boston housing data set.

(a) To begin, load in the `Boston` data set. The Boston data set is
part of the ISLR2 library. 

```{r}
install.packages("ISLR2")
library(ISLR2)
```


Now the data set is contained in the object Boston.
```
Boston
```
Read about the data set:

```{r}
?Boston
```
How many rows are in this data set? How many columns? What do the rows and columns represent?

```{r}
head(Boston)
```
506 rows and 14 columns (with 13 variables). 

Rows represent the 506 Boston suburbs.

Columns: 

crim - per capita crime rate by town.
zn - proportion of residential land zoned for lots over 25,000 sq.ft.
indus - proportion of non-retail business acres per town.
chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox - nitrogen oxides concentration (parts per 10 million).
rm - average number of rooms per dwelling.
age - proportion of owner-occupied units built prior to 1940.
dis - weighted mean of distances to five Boston employment centres.
rad - index of accessibility to radial highways.
tax - full-value property-tax rate per $10,000.
ptratio - pupil-teacher ratio by town.
lstat - lower status of the population (percent).
medv - median value of owner-occupied homes in $1000s.

(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.

```{r}
ggplot(Boston, aes(x= age, y = crim))+
  geom_point()
```
Crime tends to increase in areas with older houses.

```{r}
ggplot(Boston, aes(x= medv, y = crim))+
  geom_point()
```
Crime tends to decrease as median home value goes up.

(c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.

Yes, housing predictors tend to be associated with the per capita crime rate. We see crime increase as house ages increase and we see crime decrease as median house values increase. 

(d) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.

```{r}
summary(Boston$crim)
which.max(Boston$crim)
range(Boston$crim)

summary(Boston$tax)
which.max(Boston$tax)
range(Boston$tax)

summary(Boston$ptratio)
which.max(Boston$ptratio)
range(Boston$ptratio)
```

Suburb 381 has the highest crime rate. Range extends far beyond the median value.

Suburb 489 has the highest tax rate. Range tends to stretch pretty far beyond the median, more than double.

Suburb 355 has the highest pupil-teacher ratio. Max/min do not extend very far beyond themedian.

(e) How many of the census tracts in this data set bound the Charles river?

```{r}
sum(Boston$chas == 1)
```
 
 35 census tracts in this data set bound the Charles River.

(f) What is the median pupil-teacher ratio among the towns in this data set?

```{r}
median(Boston$ptratio)
```

The median pupil-teacher ratio among the towns in this data set is 19.05.

(g) Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

```{r}
which.min(Boston$medv)
```
Census tract 399 has lowest median value of owner-occupied homes.

```{r}
Boston[which.min(Boston$medv),]
```
In addition to the lowest median home value, census tract 399 has a high crime rate and older homes on average 

(h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.

```{r}
sum(Boston$rm > 7)
```

64 census tracts average more than seven rooms per dwelling.

```{r}
sum(Boston$rm > 8)
```

13 census tracts that average more than eight rooms per dwelling.

```{r}
summary(Boston[Boston$rm > 8,])
```

It looks like dwellings with more than 8 rooms tend to be older and have higher crime rates.
